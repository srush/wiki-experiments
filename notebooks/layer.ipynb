{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano\n",
      "from theano import config\n",
      "import theano.tensor as T\n",
      "import numpy as np\n",
      "import numpy.random\n",
      "from theano.tensor.shared_randomstreams import RandomStreams"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 178
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Words\n",
      "D = 1000 \n",
      "K = 100\n",
      "X = T.dmatrix(\"X\")\n",
      "y = T.dvector(\"y\")"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 179
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.dot(np.ones((10, 15)), np.ones((15, 20))).shape"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(10, 20)\n"
       ]
      }
     ],
     "prompt_number": 180
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class HiddenLayer:\n",
      "    def __init__(self, n_input, n_output, trng):\n",
      "        self.W = theano.shared(numpy.random.random((n_input, n_output))-0.5)\n",
      "        self.b = theano.shared(numpy.random.random((n_output, ))-0.5)\n",
      "\n",
      "        self.b_prime = theano.shared(numpy.random.random((n_input,))-0.5)\n",
      "        self.W_prime = self.W.T\n",
      "\n",
      "        self.n_input = n_input\n",
      "        self.n_output = n_output\n",
      "\n",
      "        self.trng = trng\n",
      "\n",
      "    def nonlinearity(self, y):\n",
      "        return T.tanh(y)\n",
      "\n",
      "    def apply(self, X):\n",
      "        return self.nonlinearity(T.dot(X, self.W) + self.b)\n",
      "\n",
      "    def reverse(self, Y):\n",
      "        return (T.dot(Y, self.W_prime) + self.b_prime)\n",
      "\n",
      "    def auto_encoder_loss(self, X):\n",
      "        X_noise = self.noise_input(X)\n",
      "        X_hat = self.reverse(self.apply(X_noise))\n",
      "        \n",
      "        # Cross-entropy\n",
      "        return T.nnet.binary_crossentropy(X, X_hat).mean()\n",
      "\n",
      "    def noise_input(self, X):\n",
      "        rv_b = self.trng.binomial(X.shape, 1, 0.7)\n",
      "        return  X * rv_b"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 181
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trng = RandomStreams(seed = 1)\n",
      "a = HiddenLayer(10, 5, trng)\n",
      "a.auto_encoder_loss(X)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 182,
       "text": [
        "Elemwise{true_div,no_inplace}.0"
       ]
      }
     ],
     "prompt_number": 182
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loss, X_hat = a.auto_encoder_loss(X)\n",
      "g1 = T.grad(loss, a.W)\n",
      "g2 = T.grad(loss, a.b)\n",
      "g3 = T.grad(loss, a.b_prime)"
     ],
     "language": "python",
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "TensorType does not support iteration. Maybe you are using builtin.sum instead of theano.tensor.sum? (Maybe .max?)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-183-c9593c79ed7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_encoder_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mg2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mg3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_prime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/tensor/var.pyc\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m             \u001b[1;31m# This prevents accidental iteration via builtin.sum(self)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m             raise TypeError(('TensorType does not support iteration. '\n\u001b[0m\u001b[0;32m    423\u001b[0m                              \u001b[1;34m'Maybe you are using builtin.sum instead of '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m                              'theano.tensor.sum? (Maybe .max?)'))\n",
        "\u001b[1;31mTypeError\u001b[0m: TensorType does not support iteration. Maybe you are using builtin.sum instead of theano.tensor.sum? (Maybe .max?)"
       ]
      }
     ],
     "prompt_number": 183
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grad_fn = theano.function([X], [loss, X_hat], \n",
      "                updates=[(a.W, a.W - 0.1 * g1),\n",
      "                         (a.b, a.b - 0.1 * g2),\n",
      "                         (a.b_prime, a.b_prime - 0.1 * g3)])"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_data = np.zeros((50, 10), dtype =float64)\n",
      "X_data[:25, 1] = 1\n",
      "X_data[:25, 6] = 1\n",
      "\n",
      "X_data[25:, 2] = 1\n",
      "X_data[25:, 7] = 1"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_, b = grad_fn(X_data)\n",
      "\n",
      "theano."
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#(X_data - b)**2"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 177,
       "text": [
        "array([[  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.49999976e-01,   2.50000014e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.49999975e-01,   2.50000013e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16],\n",
        "       [  5.83771556e-17,   2.50000024e-01,   2.49999986e-01,\n",
        "          3.84938399e-16,   3.50009222e-16,   4.05760135e-16,\n",
        "          2.50000025e-01,   2.49999987e-01,   3.50911614e-16,\n",
        "          4.01721428e-16]])"
       ]
      }
     ],
     "prompt_number": 177
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a.b.get_value().shape"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "(20,)"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trng.binomial((10,), 1, 0.5).eval()"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "array([1, 0, 1, 1, 1, 0, 0, 0, 0, 0])"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = T.add(T.dot(a.W, X).T,  a.b)\n",
      "m = theano.function([X], p)\n",
      "m(np.ones((10,2), dtype=float64)).shape\n",
      "#m().shape"
     ],
     "language": "python",
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Shape mismatch: x has 10 cols (and 2 rows) but y has 20 rows (and 10 cols)\nApply node that caused the error: Dot22(X.T, InplaceDimShuffle{1,0}.0)\nInputs shapes: [(2, 10), (20, 10)]\nInputs strides: [(8, 16), (8, 160)]\nInputs types: [TensorType(float64, matrix), TensorType(float64, matrix)]\nUse the Theano flag 'exception_verbosity=high' for a debugprint of this apply node.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-57-c03dce471ed3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#m().shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    586\u001b[0m                     \u001b[1;31m# For the CVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m                     gof.vm.raise_with_op(self.fn.nodes[self.fn.position_of_error],\n\u001b[1;32m--> 588\u001b[1;33m                                          self.fn.thunks[self.fn.position_of_error])\n\u001b[0m\u001b[0;32m    589\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m                     \u001b[1;31m# For the c linker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: Shape mismatch: x has 10 cols (and 2 rows) but y has 20 rows (and 10 cols)\nApply node that caused the error: Dot22(X.T, InplaceDimShuffle{1,0}.0)\nInputs shapes: [(2, 10), (20, 10)]\nInputs strides: [(8, 16), (8, 160)]\nInputs types: [TensorType(float64, matrix), TensorType(float64, matrix)]\nUse the Theano flag 'exception_verbosity=high' for a debugprint of this apply node."
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "apply_fn = theano.function([X], a.apply(X))\n",
      "reverse_fn = theano.function([X], a.reverse(X))"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b = apply_fn(np.ones((2,10), dtype=float64))\n",
      "reverse_fn(b)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 80,
       "text": [
        "array([[ 11.79578686,   9.9490711 ,  10.12958084,  12.35463244,\n",
        "          8.84372046,  10.30262758,   8.69466235,  11.53840181,\n",
        "         10.92183016,   7.17159709],\n",
        "       [ 11.79578686,   9.9490711 ,  10.12958084,  12.35463244,\n",
        "          8.84372046,  10.30262758,   8.69466235,  11.53840181,\n",
        "         10.92183016,   7.17159709]])"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loss_fn(np.ones((2,10), dtype=float64))"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 78,
       "text": [
        "array(734940.0971078795)"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loss_fn = theano.function([X], a.auto_encoder_loss(X))\n",
      "#loss_fn()"
     ],
     "language": "python",
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "Missing required input: X",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-77-c27e8d627572>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_encoder_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m                     raise TypeError(\"Missing required input: %s\" %\n\u001b[0;32m    564\u001b[0m                                     getattr(self.inv_finder[c], 'variable',\n\u001b[1;32m--> 565\u001b[1;33m                                             self.inv_finder[c]))\n\u001b[0m\u001b[0;32m    566\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprovided\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m                     raise TypeError(\"Multiple values for input: %s\" %\n",
        "\u001b[1;31mTypeError\u001b[0m: Missing required input: X"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "perturb_fn = theano.function([X], a.noise_input(X))"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#apply_fn(np.ones((10,2), dtype=float64))"
     ],
     "language": "python",
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "('Bad input argument to theano function at index 0(0-based)', 'TensorType(float32, matrix) cannot store a value of dtype float64 without risking loss of precision. If you do not mind this loss, you can: 1) explicitly cast your data to float32, or 2) set \"allow_input_downcast=True\" when calling \"function\".', array([[ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.]]))",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-136-ee5bb936e208>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mapply_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m                         s.storage[0] = s.type.filter(arg, strict=s.strict,\n\u001b[1;32m--> 497\u001b[1;33m                                 allow_downcast=s.allow_downcast)\n\u001b[0m\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[0;32m    117\u001b[0m                             \u001b[1;34m'\"function\".'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                             % (self, data.dtype, self.dtype))\n\u001b[1;32m--> 119\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m                 elif (allow_downcast is None and\n\u001b[0;32m    121\u001b[0m                         \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: ('Bad input argument to theano function at index 0(0-based)', 'TensorType(float32, matrix) cannot store a value of dtype float64 without risking loss of precision. If you do not mind this loss, you can: 1) explicitly cast your data to float32, or 2) set \"allow_input_downcast=True\" when calling \"function\".', array([[ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.],\n       [ 1.,  1.]]))"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loss_fn(np.ones((10, 5), dtype=float32))"
     ],
     "language": "python",
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Shape mismatch: x has 5 cols (and 10 rows) but y has 10 rows (and 20 cols)\nApply node that caused the error: Dot22(Elemwise{mul,no_inplace}.0, <TensorType(float64, matrix)>)\nInputs shapes: [(10, 5), (10, 20)]\nInputs strides: [(40, 8), (160, 8)]\nInputs types: [TensorType(float64, matrix), TensorType(float64, matrix)]\nUse the Theano flag 'exception_verbosity=high' for a debugprint of this apply node.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-144-547767599c9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    586\u001b[0m                     \u001b[1;31m# For the CVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m                     gof.vm.raise_with_op(self.fn.nodes[self.fn.position_of_error],\n\u001b[1;32m--> 588\u001b[1;33m                                          self.fn.thunks[self.fn.position_of_error])\n\u001b[0m\u001b[0;32m    589\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m                     \u001b[1;31m# For the c linker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: Shape mismatch: x has 5 cols (and 10 rows) but y has 10 rows (and 20 cols)\nApply node that caused the error: Dot22(Elemwise{mul,no_inplace}.0, <TensorType(float64, matrix)>)\nInputs shapes: [(10, 5), (10, 20)]\nInputs strides: [(40, 8), (160, 8)]\nInputs types: [TensorType(float64, matrix), TensorType(float64, matrix)]\nUse the Theano flag 'exception_verbosity=high' for a debugprint of this apply node."
       ]
      }
     ],
     "prompt_number": 144
    }
   ]
  }
 ]
}